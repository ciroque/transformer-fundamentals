{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35eba72f",
   "metadata": {},
   "source": [
    "\n",
    "    # Transformer Fundamentals – Guided Notebook 03 — Scoring (Dot Product & Scaling)\n",
    "    **Date:** 2025-10-29  \n",
    "    **Style:** Guided, hands-on; from-scratch first, then frameworks; interactive visuals\n",
    "\n",
    "    ## Learning Objectives\n",
    "\n",
    "- Understand why dot-product similarity and scaling by sqrt(d_k) are used.\n",
    "- Explore numerical stability with/without scaling.\n",
    "- Visualize how score magnitudes change with dimensionality.\n",
    "\n",
    "\n",
    "    ## TL;DR\n",
    "    Scaling by sqrt(d_k) keeps softmax in a sensible range as dimensions grow, aiding stable gradients and sharper-yet-controlled attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca6e23",
   "metadata": {},
   "source": [
    "## Concept Overview\n",
    "- Dot products grow in magnitude with vector dimension; scaling normalizes score variance.\n",
    "- Unscaled scores can saturate softmax → vanishing gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a57160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [setup] Environment check & minimal installs (run once per kernel)\n",
    "# Target: Python 3.12.12, PyTorch 2.5+, transformers 4.44+, datasets 3+, ipywidgets 8+, matplotlib 3.8+\n",
    "import sys, platform, subprocess, os\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optional: uncomment to install/upgrade on this machine (internet required)\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"torch>=2.5\" \"transformers>=4.44\" \"datasets>=3.0.0\" \"ipywidgets>=8.1.0\" \"matplotlib>=3.8\" \"umap-learn>=0.5.6\"\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not available yet:\", e)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, interactive\n",
    "    print(\"ipywidgets:\", widgets.__version__)\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available yet:\", e)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [utils] Small helpers used throughout\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cosine_sim(a, b, eps=1e-9):\n",
    "    a_norm = a / (np.linalg.norm(a, axis=-1, keepdims=True) + eps)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=-1, keepdims=True) + eps)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def show_heatmap(mat, xticklabels=None, yticklabels=None, title=\"\"):\n",
    "    plt.figure()\n",
    "    plt.imshow(mat, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    if xticklabels is not None: plt.xticks(range(len(xticklabels)), xticklabels, rotation=45, ha=\"right\")\n",
    "    if yticklabels is not None: plt.yticks(range(len(yticklabels)), yticklabels)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57078682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [experiment] Scaling effect demo (NumPy)\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def scaling_demo(d_k=16):\n",
    "    T = 10\n",
    "    X = np.random.randn(T, d_k)\n",
    "    Q = X.copy()\n",
    "    K = X.copy()\n",
    "    scores_unscaled = Q @ K.T\n",
    "    scores_scaled = scores_unscaled / np.sqrt(d_k)\n",
    "    print(\"d_k:\", d_k)\n",
    "    print(\"scores_unscaled std:\", scores_unscaled.std())\n",
    "    print(\"scores_scaled   std:\", scores_scaled.std())\n",
    "    show_heatmap(softmax(scores_unscaled, -1), title=\"Softmax(Unscaled)\")\n",
    "    show_heatmap(softmax(scores_scaled, -1),   title=\"Softmax(Scaled)\")\n",
    "\n",
    "widgets.interact(scaling_demo, d_k=widgets.IntSlider(min=4, max=256, step=4, value=16))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9829c7b",
   "metadata": {},
   "source": [
    "### Framework Tie-in\n",
    "- Inspect PyTorch’s `nn.MultiheadAttention` source or confirm scaling behavior by probing outputs as you vary `embed_dim`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65531127",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Bonus: Multilingual Extension\n",
    "- Swap the tokenizer/model for a multilingual variant (e.g., `bert-base-multilingual-cased` or `xlm-roberta-base`).\n",
    "- Repeat a small slice of the notebook (tokenization, attention map) on non-English sentences and compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0755f774",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Reflection & Next Steps\n",
    "- What changed when you tweaked dimensions, temperatures, or prompts?\n",
    "- Where did the attention concentrate, and did it match your intuition?\n",
    "- Re-run the interactive widgets on your own text.\n",
    "- Save a copy of the figures that best illustrate your understanding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

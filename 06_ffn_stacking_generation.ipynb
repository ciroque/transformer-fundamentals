{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78320b2d",
   "metadata": {},
   "source": [
    "\n",
    "    # Transformer Fundamentals – Guided Notebook 06 — Feed-Forward, Stacking & Generation Loop\n",
    "    **Date:** 2025-10-29  \n",
    "    **Style:** Guided, hands-on; from-scratch first, then frameworks; interactive visuals\n",
    "\n",
    "    ## Learning Objectives\n",
    "\n",
    "- Understand residual connections, layer normalization, and position-wise feed-forward networks.\n",
    "- Stack layers; observe how representations evolve.\n",
    "- Run a tiny generation loop (causal masking) on a toy corpus; compare GPT-2 behavior.\n",
    "\n",
    "\n",
    "    ## TL;DR\n",
    "    Transformer layers alternate attention and MLPs with residuals and layer norms; generation repeats forward passes token by token with causal masks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ee04a",
   "metadata": {},
   "source": [
    "## Concept Overview\n",
    "- Each layer: LN → (Attention → Residual) → LN → (MLP → Residual).\n",
    "- Causal masking prevents tokens from attending to the future in decoder-only models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a57160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [setup] Environment check & minimal installs (run once per kernel)\n",
    "# Target: Python 3.12.12, PyTorch 2.5+, transformers 4.44+, datasets 3+, ipywidgets 8+, matplotlib 3.8+\n",
    "import sys, platform, subprocess, os\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optional: uncomment to install/upgrade on this machine (internet required)\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"torch>=2.5\" \"transformers>=4.44\" \"datasets>=3.0.0\" \"ipywidgets>=8.1.0\" \"matplotlib>=3.8\" \"umap-learn>=0.5.6\"\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not available yet:\", e)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, interactive\n",
    "    print(\"ipywidgets:\", widgets.__version__)\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available yet:\", e)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [utils] Small helpers used throughout\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cosine_sim(a, b, eps=1e-9):\n",
    "    a_norm = a / (np.linalg.norm(a, axis=-1, keepdims=True) + eps)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=-1, keepdims=True) + eps)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def show_heatmap(mat, xticklabels=None, yticklabels=None, title=\"\"):\n",
    "    plt.figure()\n",
    "    plt.imshow(mat, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    if xticklabels is not None: plt.xticks(range(len(xticklabels)), xticklabels, rotation=45, ha=\"right\")\n",
    "    if yticklabels is not None: plt.yticks(range(len(yticklabels)), yticklabels)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [from-scratch] Minimal transformer block (NumPy, illustrative – not optimized)\n",
    "T, d_model, nheads = 8, 32, 4\n",
    "d_ff = 64\n",
    "d_k = d_model // nheads\n",
    "\n",
    "def layer_norm(x, eps=1e-5):\n",
    "    mu = x.mean(-1, keepdims=True)\n",
    "    sigma2 = ((x - mu)**2).mean(-1, keepdims=True)\n",
    "    return (x - mu) / np.sqrt(sigma2 + eps)\n",
    "\n",
    "X = np.random.randn(T, d_model) * 0.1\n",
    "\n",
    "# Attention params (shared within block)\n",
    "W_Q = np.random.randn(d_model, d_model) * 0.1\n",
    "W_K = np.random.randn(d_model, d_model) * 0.1\n",
    "W_V = np.random.randn(d_model, d_model) * 0.1\n",
    "W_O = np.random.randn(d_model, d_model) * 0.1\n",
    "\n",
    "# FFN params\n",
    "W1 = np.random.randn(d_model, d_ff) * 0.1\n",
    "b1 = np.zeros(d_ff)\n",
    "W2 = np.random.randn(d_ff, d_model) * 0.1\n",
    "b2 = np.zeros(d_model)\n",
    "\n",
    "def mha_block(x):\n",
    "    def split_heads(M):\n",
    "        return M.reshape(T, nheads, d_k).transpose(1,0,2)\n",
    "    Q = x @ W_Q; K = x @ W_K; V = x @ W_V\n",
    "    Qh, Kh, Vh = map(split_heads, (Q,K,V))\n",
    "    heads = []\n",
    "    for h in range(nheads):\n",
    "        scores = (Qh[h] @ Kh[h].T) / np.sqrt(d_k)\n",
    "        # causal mask (upper triangle = -inf)\n",
    "        mask = np.triu(np.ones_like(scores), k=1)*1e9\n",
    "        scores = scores - mask\n",
    "        attn = softmax(scores, -1)\n",
    "        heads.append(attn @ Vh[h])\n",
    "    H = np.stack(heads, axis=1).reshape(T, d_model)\n",
    "    return H @ W_O\n",
    "\n",
    "# LayerNorm → MHA → Residual\n",
    "y = layer_norm(X)\n",
    "y = X + mha_block(y)\n",
    "# LayerNorm → FFN → Residual\n",
    "z = layer_norm(y)\n",
    "z = y + np.maximum(0, z @ W1 + b1) @ W2 + b2   # ReLU\n",
    "print(\"Block output shape:\", z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [framework] Tiny generation loop with GPT-2 (HF Transformers)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "mdl.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mdl.to(device)\n",
    "\n",
    "prompt = \"In a small village,\"\n",
    "ids = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    gen = mdl.generate(**ids, max_length=40, temperature=0.8, top_p=0.95, do_sample=True)\n",
    "print(tok.decode(gen[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d45050",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Bonus: Multilingual Extension\n",
    "- Swap the tokenizer/model for a multilingual variant (e.g., `bert-base-multilingual-cased` or `xlm-roberta-base`).\n",
    "- Repeat a small slice of the notebook (tokenization, attention map) on non-English sentences and compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d224cf9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Reflection & Next Steps\n",
    "- What changed when you tweaked dimensions, temperatures, or prompts?\n",
    "- Where did the attention concentrate, and did it match your intuition?\n",
    "- Re-run the interactive widgets on your own text.\n",
    "- Save a copy of the figures that best illustrate your understanding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76539b7f",
   "metadata": {},
   "source": [
    "\n",
    "    # Transformer Fundamentals – Guided Notebook 05 — Weighted Sum (Attention Output & Multi-Head)\n",
    "    **Date:** 2025-10-29  \n",
    "    **Style:** Guided, hands-on; from-scratch first, then frameworks; interactive visuals\n",
    "\n",
    "    ## Learning Objectives\n",
    "\n",
    "- Compute attention outputs as weighted sums of V.\n",
    "- Build multi-head attention by splitting heads and concatenating.\n",
    "- Visualize per-head patterns; discuss why multiple heads help.\n",
    "\n",
    "\n",
    "    ## TL;DR\n",
    "    Attention output is a weighted sum of values; multiple heads learn diverse relational patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79511af4",
   "metadata": {},
   "source": [
    "## Concept Overview\n",
    "- Single-head: `Attn = softmax(QK^T / sqrt(d_k))`, `Out = Attn V`.\n",
    "- Multi-head: split into heads, apply attention in parallel, concat, then project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a57160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [setup] Environment check & minimal installs (run once per kernel)\n",
    "# Target: Python 3.12.12, PyTorch 2.5+, transformers 4.44+, datasets 3+, ipywidgets 8+, matplotlib 3.8+\n",
    "import sys, platform, subprocess, os\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optional: uncomment to install/upgrade on this machine (internet required)\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"torch>=2.5\" \"transformers>=4.44\" \"datasets>=3.0.0\" \"ipywidgets>=8.1.0\" \"matplotlib>=3.8\" \"umap-learn>=0.5.6\"\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not available yet:\", e)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, interactive\n",
    "    print(\"ipywidgets:\", widgets.__version__)\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available yet:\", e)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [utils] Small helpers used throughout\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cosine_sim(a, b, eps=1e-9):\n",
    "    a_norm = a / (np.linalg.norm(a, axis=-1, keepdims=True) + eps)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=-1, keepdims=True) + eps)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def show_heatmap(mat, xticklabels=None, yticklabels=None, title=\"\"):\n",
    "    plt.figure()\n",
    "    plt.imshow(mat, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    if xticklabels is not None: plt.xticks(range(len(xticklabels)), xticklabels, rotation=45, ha=\"right\")\n",
    "    if yticklabels is not None: plt.yticks(range(len(yticklabels)), yticklabels)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04317ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [from-scratch] Multi-head attention (NumPy, minimal)\n",
    "T, d_model, nheads = 8, 32, 4\n",
    "d_k = d_model // nheads\n",
    "\n",
    "X = np.random.randn(T, d_model) * 0.2\n",
    "W_Q = np.random.randn(d_model, d_model) * 0.1\n",
    "W_K = np.random.randn(d_model, d_model) * 0.1\n",
    "W_V = np.random.randn(d_model, d_model) * 0.1\n",
    "W_O = np.random.randn(d_model, d_model) * 0.1\n",
    "\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "def split_heads(M):\n",
    "    return M.reshape(T, nheads, d_k).transpose(1,0,2)  # [heads, T, d_k]\n",
    "\n",
    "Qh, Kh, Vh = map(split_heads, (Q,K,V))\n",
    "heads = []\n",
    "for h in range(nheads):\n",
    "    scores = (Qh[h] @ Kh[h].T) / np.sqrt(d_k)\n",
    "    attn = softmax(scores, -1)\n",
    "    heads.append(attn @ Vh[h])\n",
    "H = np.stack(heads, axis=1).reshape(T, d_model)\n",
    "Out = H @ W_O\n",
    "\n",
    "print(\"Out shape:\", Out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [visualize] Per-head attention heatmaps (PyTorch)\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "T, d_model, nheads = 10, 64, 4\n",
    "mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=nheads, batch_first=True)\n",
    "x = torch.randn(1, T, d_model)\n",
    "out, weights = mha(x, x, x, need_weights=True)  # weights: [1, heads, T, T]\n",
    "weights = weights[0].detach().numpy()\n",
    "\n",
    "for h in range(nheads):\n",
    "    show_heatmap(weights[h], title=f\"Head {h} Attention\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5ca43",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Bonus: Multilingual Extension\n",
    "- Swap the tokenizer/model for a multilingual variant (e.g., `bert-base-multilingual-cased` or `xlm-roberta-base`).\n",
    "- Repeat a small slice of the notebook (tokenization, attention map) on non-English sentences and compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e2f98",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Reflection & Next Steps\n",
    "- What changed when you tweaked dimensions, temperatures, or prompts?\n",
    "- Where did the attention concentrate, and did it match your intuition?\n",
    "- Re-run the interactive widgets on your own text.\n",
    "- Save a copy of the figures that best illustrate your understanding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a2b090",
   "metadata": {},
   "source": [
    "\n",
    "    # Transformer Fundamentals – Guided Notebook 04 — Softmax Weighting\n",
    "    **Date:** 2025-10-29  \n",
    "    **Style:** Guided, hands-on; from-scratch first, then frameworks; interactive visuals\n",
    "\n",
    "    ## Learning Objectives\n",
    "\n",
    "- Deepen intuition for softmax as a temperature-controlled probability distribution.\n",
    "- Explore temperature τ and its effect on focus vs diffusion.\n",
    "- Observe sparsity patterns and entropy of attention weights.\n",
    "\n",
    "\n",
    "    ## TL;DR\n",
    "    Softmax converts scores into a probability distribution; temperature controls confidence vs exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17bc9a",
   "metadata": {},
   "source": [
    "## Concept Overview\n",
    "- Softmax emphasizes relative differences; temperature τ < 1 sharpens, τ > 1 smooths.\n",
    "- Entropy is a useful summary of distribution sharpness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a57160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [setup] Environment check & minimal installs (run once per kernel)\n",
    "# Target: Python 3.12.12, PyTorch 2.5+, transformers 4.44+, datasets 3+, ipywidgets 8+, matplotlib 3.8+\n",
    "import sys, platform, subprocess, os\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optional: uncomment to install/upgrade on this machine (internet required)\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"torch>=2.5\" \"transformers>=4.44\" \"datasets>=3.0.0\" \"ipywidgets>=8.1.0\" \"matplotlib>=3.8\" \"umap-learn>=0.5.6\"\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not available yet:\", e)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, interactive\n",
    "    print(\"ipywidgets:\", widgets.__version__)\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available yet:\", e)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [utils] Small helpers used throughout\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cosine_sim(a, b, eps=1e-9):\n",
    "    a_norm = a / (np.linalg.norm(a, axis=-1, keepdims=True) + eps)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=-1, keepdims=True) + eps)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def show_heatmap(mat, xticklabels=None, yticklabels=None, title=\"\"):\n",
    "    plt.figure()\n",
    "    plt.imshow(mat, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    if xticklabels is not None: plt.xticks(range(len(xticklabels)), xticklabels, rotation=45, ha=\"right\")\n",
    "    if yticklabels is not None: plt.yticks(range(len(yticklabels)), yticklabels)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685148b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [experiment] Temperature & entropy exploration (NumPy)\n",
    "import ipywidgets as widgets\n",
    "def entropy(p, eps=1e-12):\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    return -np.sum(p * np.log(p), axis=-1).mean()\n",
    "\n",
    "def softmax_temp(x, tau=1.0, axis=-1):\n",
    "    x = x / max(tau, 1e-6)\n",
    "    return softmax(x, axis=axis)\n",
    "\n",
    "def temp_demo(tau=1.0):\n",
    "    T, d = 10, 16\n",
    "    X = np.random.randn(T, d)\n",
    "    A = X @ X.T  # symmetric scores\n",
    "    P = softmax_temp(A, tau=tau, axis=-1)\n",
    "    print(\"Temperature:\", tau, \"| mean entropy:\", entropy(P))\n",
    "    show_heatmap(P, title=f\"Softmax with Temperature={tau}\")\n",
    "\n",
    "widgets.interact(temp_demo, tau=widgets.FloatLogSlider(base=10, min=-2, max=1, step=0.05, value=1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113801a",
   "metadata": {},
   "source": [
    "### Framework Tie-in\n",
    "- In generation APIs, temperature is applied to logits before sampling; it changes token selection confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5e9c4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Bonus: Multilingual Extension\n",
    "- Swap the tokenizer/model for a multilingual variant (e.g., `bert-base-multilingual-cased` or `xlm-roberta-base`).\n",
    "- Repeat a small slice of the notebook (tokenization, attention map) on non-English sentences and compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8bd3c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Reflection & Next Steps\n",
    "- What changed when you tweaked dimensions, temperatures, or prompts?\n",
    "- Where did the attention concentrate, and did it match your intuition?\n",
    "- Re-run the interactive widgets on your own text.\n",
    "- Save a copy of the figures that best illustrate your understanding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bded8c68",
   "metadata": {},
   "source": [
    "\n",
    "    # Transformer Fundamentals – Guided Notebook 02 — Attention Mechanism (Q/K/V)\n",
    "    **Date:** 2025-10-29  \n",
    "    **Style:** Guided, hands-on; from-scratch first, then frameworks; interactive visuals\n",
    "\n",
    "    ## Learning Objectives\n",
    "\n",
    "- Compute Q, K, V projections from token embeddings.\n",
    "- Build scaled dot-product attention from scratch.\n",
    "- Inspect shapes and parameters for single-head vs multi-head attention.\n",
    "- Compare a NumPy implementation to PyTorch’s nn.MultiheadAttention.\n",
    "\n",
    "\n",
    "    ## TL;DR\n",
    "    Q queries, K keys, V values: attention selects information by weighting V according to Q·K similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5785d",
   "metadata": {},
   "source": [
    "## Concept Overview\n",
    "From embeddings `X` (shape `[T, d_model]`), linear layers produce `Q = XW_Q`, `K = XW_K`, `V = XW_V`.\n",
    "Attention weights are computed via scaled dot product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a57160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [setup] Environment check & minimal installs (run once per kernel)\n",
    "# Target: Python 3.12.12, PyTorch 2.5+, transformers 4.44+, datasets 3+, ipywidgets 8+, matplotlib 3.8+\n",
    "import sys, platform, subprocess, os\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optional: uncomment to install/upgrade on this machine (internet required)\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"torch>=2.5\" \"transformers>=4.44\" \"datasets>=3.0.0\" \"ipywidgets>=8.1.0\" \"matplotlib>=3.8\" \"umap-learn>=0.5.6\"\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not available yet:\", e)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, interactive\n",
    "    print(\"ipywidgets:\", widgets.__version__)\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available yet:\", e)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [utils] Small helpers used throughout\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cosine_sim(a, b, eps=1e-9):\n",
    "    a_norm = a / (np.linalg.norm(a, axis=-1, keepdims=True) + eps)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=-1, keepdims=True) + eps)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def show_heatmap(mat, xticklabels=None, yticklabels=None, title=\"\"):\n",
    "    plt.figure()\n",
    "    plt.imshow(mat, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    if xticklabels is not None: plt.xticks(range(len(xticklabels)), xticklabels, rotation=45, ha=\"right\")\n",
    "    if yticklabels is not None: plt.yticks(range(len(yticklabels)), yticklabels)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2766e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [from-scratch] Single-head attention (NumPy)\n",
    "T, d_model, d_k = 6, 16, 16\n",
    "X = np.random.randn(T, d_model) * 0.2\n",
    "W_Q = np.random.randn(d_model, d_k) * 0.1\n",
    "W_K = np.random.randn(d_model, d_k) * 0.1\n",
    "W_V = np.random.randn(d_model, d_k) * 0.1\n",
    "\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "scores = (Q @ K.T) / np.sqrt(d_k)   # [T, T]\n",
    "attn = softmax(scores, axis=-1)\n",
    "out = attn @ V                      # [T, d_k]\n",
    "\n",
    "print(\"scores shape:\", scores.shape, \"attn shape:\", attn.shape, \"out shape:\", out.shape)\n",
    "show_heatmap(attn, title=\"Attention Weights (Toy)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [framework] PyTorch MultiheadAttention check\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "T, d_model, nheads = 6, 32, 4\n",
    "mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=nheads, batch_first=True)\n",
    "x = torch.randn(1, T, d_model)\n",
    "out, w = mha(x, x, x, need_weights=True)\n",
    "print(\"out:\", out.shape, \"weights:\", w.shape)  # weights: [1, heads, T, T]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2c12b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Bonus: Multilingual Extension\n",
    "- Swap the tokenizer/model for a multilingual variant (e.g., `bert-base-multilingual-cased` or `xlm-roberta-base`).\n",
    "- Repeat a small slice of the notebook (tokenization, attention map) on non-English sentences and compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4e5b1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Reflection & Next Steps\n",
    "- What changed when you tweaked dimensions, temperatures, or prompts?\n",
    "- Where did the attention concentrate, and did it match your intuition?\n",
    "- Re-run the interactive widgets on your own text.\n",
    "- Save a copy of the figures that best illustrate your understanding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

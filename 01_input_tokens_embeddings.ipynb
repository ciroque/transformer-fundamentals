{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8e3b84",
   "metadata": {},
   "source": [
    "# Transformer Fundamentals – Guided Notebook 01 — Input Tokens (Tokenization & Embeddings)\n",
    "**Date:** 2025-10-29\n",
    "**Style:** Guided, hands-on; from-scratch first, then frameworks; interactive visuals\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand subword tokenization (BPE/WordPiece) and why it’s used.\n",
    "- Map tokens to vectors via embedding tables.\n",
    "- Visualize token embeddings with [Principal Component Analysis, or PCA](./GLOSSARY.md#pca); explore proximity and analogies.\n",
    "- Connect toy NumPy embeddings to HF tokenizers and real models (GPT-2, BERT).\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "Text → tokens → indices → embedding vectors. Tokenization shapes what the model can represent;\n",
    "embeddings give each token a learned coordinate in vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bccbaf",
   "metadata": {},
   "source": [
    "## Concept Overview\n",
    "\n",
    "### Tokenization splits text into subword units to balance vocabulary size and coverage\n",
    "Language models can’t operate directly on raw text — they need discrete units called *tokens*.\n",
    "Early systems used full words, but that led to extremely large vocabularies and poor handling of rare or new words.\n",
    "Modern models instead use **subword tokenization**, such as *Byte Pair Encoding (BPE)* or *WordPiece*, which splits words into smaller chunks like `\"play\"`, `\"##ing\"`, `\"##ed\"`.\n",
    "This approach strikes a balance between:\n",
    "- **Coverage:** the ability to represent any input text, including unseen words\n",
    "- **Compactness:** keeping the vocabulary small enough for efficient training and inference\n",
    "\n",
    "For example:\n",
    "> `\"playing\"` → `[\"play\", \"##ing\"]`\n",
    "> `\"cats\"` → `[\"cat\", \"##s\"]`\n",
    "\n",
    "These consistent subword fragments allow the model to generalize across similar forms of words without memorizing every variant.\n",
    "\n",
    "---\n",
    "\n",
    "### Each token id indexes into an embedding matrix `E` with shape `[vocab_size, d_model]`\n",
    "Once tokenized, text becomes a list of integer IDs — each representing a position in the vocabulary.\n",
    "These IDs map into an **embedding matrix** `E`, a learnable lookup table where each row corresponds to a token’s vector representation.\n",
    "If your vocabulary has 50,000 tokens and the model’s hidden size (`d_model`) is 768, then `E` has shape `[50000, 768]`.\n",
    "When the model processes a sentence, it retrieves the embeddings for the tokens it sees:\n",
    "\n",
    "\\[\n",
    "\\text{Embeddings} = E[\\text{token\\_ids}]\n",
    "\\]\n",
    "\n",
    "Each token’s embedding acts like its *coordinate* in a high-dimensional semantic space.\n",
    "During training, these vectors are updated so that tokens appearing in similar contexts end up close to each other in this space.\n",
    "\n",
    "---\n",
    "\n",
    "### Similar tokens often cluster in embedding space\n",
    "Because embeddings capture contextual meaning, similar or related words develop similar vector representations.\n",
    "For instance:\n",
    "- `\"king\"` and `\"queen\"` end up close together\n",
    "- `\"cat\"` and `\"dog\"` might form a cluster separate from `\"car\"` or `\"tree\"`\n",
    "\n",
    "We can visualize this by projecting embeddings into 2D (e.g., using PCA or UMAP).\n",
    "You’ll typically see clear groupings: plural forms, verb tenses, or semantically related concepts cluster naturally.\n",
    "\n",
    "This property makes embeddings useful beyond Transformers — they’re the backbone for many semantic similarity and retrieval systems.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "Tokenization breaks text into manageable, reusable pieces.\n",
    "Embeddings turn those pieces into vectors that capture relationships between words.\n",
    "Together, they form the foundation upon which attention and all subsequent Transformer layers operate.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "64a57160",
   "metadata": {},
   "source": [
    "\n",
    "# %% [setup] Environment check & minimal installs (run once per kernel)\n",
    "# Target: Python 3.12.12, PyTorch 2.5+, transformers 4.44+, datasets 3+, ipywidgets 8+, matplotlib 3.8+\n",
    "import sys, platform, subprocess, os\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optional: uncomment to install/upgrade on this machine (internet required)\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"torch>=2.5\" \"transformers>=4.44\" \"datasets>=3.0.0\" \"ipywidgets>=8.1.0\" \"matplotlib>=3.8\" \"umap-learn>=0.5.6\"\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "        print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not available yet:\", e)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, interactive\n",
    "    print(\"ipywidgets:\", widgets.__version__)\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available yet:\", e)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "194230d5",
   "metadata": {},
   "source": [
    "\n",
    "# %% [utils] Small helpers used throughout\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cosine_sim(a, b, eps=1e-9):\n",
    "    a_norm = a / (np.linalg.norm(a, axis=-1, keepdims=True) + eps)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=-1, keepdims=True) + eps)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def show_heatmap(mat, xticklabels=None, yticklabels=None, title=\"\"):\n",
    "    plt.figure()\n",
    "    plt.imshow(mat, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    if xticklabels is not None: plt.xticks(range(len(xticklabels)), xticklabels, rotation=45, ha=\"right\")\n",
    "    if yticklabels is not None: plt.yticks(range(len(yticklabels)), yticklabels)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7016139f",
   "metadata": {},
   "source": [
    "\n",
    "# %% [from-scratch] Toy BPE-like tokenization & embeddings (NumPy)\n",
    "vocab = {\"<pad>\":0, \"<unk>\":1, \"the\":2, \"cat\":3, \"##s\":4, \"sat\":5, \"on\":6, \"##ting\":7}\n",
    "d_model = 8\n",
    "E = np.random.randn(len(vocab), d_model) * 0.1  # random toy embeddings\n",
    "\n",
    "def toy_tokenize(text):\n",
    "    # minimal heuristic just for demonstration\n",
    "    parts = text.lower().split()\n",
    "    tokens = []\n",
    "    for w in parts:\n",
    "        if w in vocab: tokens.append(w)\n",
    "        elif w.endswith(\"s\") and w[:-1] in vocab:\n",
    "            tokens += [w[:-1], \"##s\"]\n",
    "        elif w.endswith(\"ting\") and w[:-5] in vocab:\n",
    "            tokens += [w[:-5], \"##ting\"]\n",
    "        else:\n",
    "            tokens.append(\"<unk>\")\n",
    "    return tokens\n",
    "\n",
    "text = \"The cat sits on the cat\"\n",
    "toks = toy_tokenize(text)\n",
    "ids = [vocab.get(t, vocab[\"<unk>\"]) for t in toks]\n",
    "vecs = E[ids]\n",
    "print(\"Tokens:\", toks)\n",
    "print(\"IDs:\", ids)\n",
    "print(\"Embeddings shape:\", vecs.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b5e9630",
   "metadata": {},
   "source": [
    "\n",
    "# %% [visualize] PCA projection of embeddings of unique tokens in this sentence\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "unique_ids = sorted(set(ids))\n",
    "X = E[unique_ids]\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "X2 = pca.transform(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X2[:,0], X2[:,1])\n",
    "for i, tid in enumerate(unique_ids):\n",
    "    token = list(vocab.keys())[list(vocab.values()).index(tid)]\n",
    "    plt.text(X2[i,0], X2[i,1], token)\n",
    "plt.title(\"Toy embedding PCA (2D)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97a3783f",
   "metadata": {},
   "source": [
    "### Framework Section: HF Tokenizers & Real Embeddings\n",
    "- Use GPT-2 and BERT tokenizers to compare vocabulary and tokenization behavior.\n",
    "- Load small models to peek at real embedding matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5175444d",
   "metadata": {},
   "source": [
    "\n",
    "# %% [framework] Hugging Face tokenizers & model embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_names = {\n",
    "    \"gpt2\": \"gpt2\",\n",
    "    \"bert\": \"bert-base-uncased\"\n",
    "}\n",
    "\n",
    "for label, m in model_names.items():\n",
    "    print(f\"--- {label.upper()} ---\")\n",
    "    tok = AutoTokenizer.from_pretrained(m)\n",
    "    sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    toks = tok.tokenize(sample)\n",
    "    ids = tok.encode(sample, add_special_tokens=True)\n",
    "    print(\"Tokens:\", toks)\n",
    "    print(\"IDs:\", ids[:12], \"...\")\n",
    "\n",
    "    mdl = AutoModel.from_pretrained(m)\n",
    "    emb = mdl.get_input_embeddings().weight.detach().cpu().numpy()\n",
    "    print(\"Embedding matrix shape:\", emb.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e9674d65",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Bonus: Multilingual Extension\n",
    "- Swap the tokenizer/model for a multilingual variant (e.g., `bert-base-multilingual-cased` or `xlm-roberta-base`).\n",
    "- Repeat a small slice of the notebook (tokenization, attention map) on non-English sentences and compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0e955",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Reflection & Next Steps\n",
    "- What changed when you tweaked dimensions, temperatures, or prompts?\n",
    "- Where did the attention concentrate, and did it match your intuition?\n",
    "- Re-run the interactive widgets on your own text.\n",
    "- Save a copy of the figures that best illustrate your understanding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
